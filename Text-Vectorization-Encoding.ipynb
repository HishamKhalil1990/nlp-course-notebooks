{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59595119",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f3aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421257f1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bcc29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_tokenizer.pkl',\n",
       " 'clean-tweets.tsv',\n",
       " 'arsentd-lev',\n",
       " 'arsentd-lev.zip',\n",
       " 'MNIST']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join(os.curdir, \"data\")\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab171d7c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b479f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Country</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Expression</th>\n",
       "      <th>Sentiment_Target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...</td>\n",
       "      <td>lebanon</td>\n",
       "      <td>personal</td>\n",
       "      <td>negative</td>\n",
       "      <td>implicit</td>\n",
       "      <td>بريق العيون</td>\n",
       "      <td>23</td>\n",
       "      <td>132</td>\n",
       "      <td>أؤمن بأن الانسان ينطفئ جماله ابتعاد يحب بريق ا...</td>\n",
       "      <td>اؤم بأن انس طفئ جمل بعد يحب برق عين خفي صبح ذب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...</td>\n",
       "      <td>jordan</td>\n",
       "      <td>sports</td>\n",
       "      <td>positive</td>\n",
       "      <td>explicit</td>\n",
       "      <td>افضل لاعب في العالم</td>\n",
       "      <td>23</td>\n",
       "      <td>141</td>\n",
       "      <td>الذاكره عندما اعتقد كريستيانو انه افضل لاعب ال...</td>\n",
       "      <td>ذكر عند عقد كريستيانو انه فضل لعب علم ككا يسي ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لا نخلو من ضغوطات الحياة. فنحن نعيش على أرض أع...</td>\n",
       "      <td>palestine</td>\n",
       "      <td>personal</td>\n",
       "      <td>neutral</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>24</td>\n",
       "      <td>133</td>\n",
       "      <td>نخلو ضغوطات الحياة فنحن نعيش أرض أعدت للبلاء و...</td>\n",
       "      <td>خلو ضغط حية فنح نعش ارض اعد بلء ولم سلم بيء وك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...</td>\n",
       "      <td>lebanon</td>\n",
       "      <td>personal</td>\n",
       "      <td>negative</td>\n",
       "      <td>explicit</td>\n",
       "      <td>مصطلحات_لبنانيه</td>\n",
       "      <td>23</td>\n",
       "      <td>135</td>\n",
       "      <td>بتوصل عالبيت بنط بقلك جيت بتقعد لتتحدث معو بقل...</td>\n",
       "      <td>وصل علب بنط بقل جيت قعد حدث معو بقل شو تقم تمش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...</td>\n",
       "      <td>palestine</td>\n",
       "      <td>personal</td>\n",
       "      <td>negative</td>\n",
       "      <td>explicit</td>\n",
       "      <td>س لم تعد كما ك</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>نصمت لتسير حياتنا يرام فالناس تعد كآنت نقيه</td>\n",
       "      <td>نصم تسر حيت يرم لنس تعد كآن نقه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet    Country     Topic  \\\n",
       "0  \"أنا أؤمن بأن الانسان ينطفئ جماله عند ابتعاد م...    lebanon  personal   \n",
       "1  من الذاكره... @3FInQe . عندما اعتقد كريستيانو ...     jordan    sports   \n",
       "2  لا نخلو من ضغوطات الحياة. فنحن نعيش على أرض أع...  palestine  personal   \n",
       "3  #مصطلحات_لبنانيه_حيرت_البشريه بتوصل عالبيت ، ب...    lebanon  personal   \n",
       "4  نصمت !! لتسير حياتنا على مً يرام فالناّس لم تع...  palestine  personal   \n",
       "\n",
       "  Sentiment Sentiment_Expression     Sentiment_Target  word_count  char_count  \\\n",
       "0  negative             implicit          بريق العيون          23         132   \n",
       "1  positive             explicit  افضل لاعب في العالم          23         141   \n",
       "2   neutral                 none                 none          24         133   \n",
       "3  negative             explicit      مصطلحات_لبنانيه          23         135   \n",
       "4  negative             explicit       س لم تعد كما ك          16          67   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  أؤمن بأن الانسان ينطفئ جماله ابتعاد يحب بريق ا...   \n",
       "1  الذاكره عندما اعتقد كريستيانو انه افضل لاعب ال...   \n",
       "2  نخلو ضغوطات الحياة فنحن نعيش أرض أعدت للبلاء و...   \n",
       "3  بتوصل عالبيت بنط بقلك جيت بتقعد لتتحدث معو بقل...   \n",
       "4        نصمت لتسير حياتنا يرام فالناس تعد كآنت نقيه   \n",
       "\n",
       "                                       clean_stemmed  \n",
       "0  اؤم بأن انس طفئ جمل بعد يحب برق عين خفي صبح ذب...  \n",
       "1  ذكر عند عقد كريستيانو انه فضل لعب علم ككا يسي ...  \n",
       "2  خلو ضغط حية فنح نعش ارض اعد بلء ولم سلم بيء وك...  \n",
       "3  وصل علب بنط بقل جيت قعد حدث معو بقل شو تقم تمش...  \n",
       "4                    نصم تسر حيت يرم لنس تعد كآن نقه  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filepath = os.path.join(data_dir, \"clean-tweets.tsv\")\n",
    "raw = pd.read_csv(filepath_or_buffer=data_filepath, sep=\"\\t\")\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b10e9",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0fbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a generic Tokenizer class\n",
    "# Different tokenizers will inherit from this class\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus: list[str]):\n",
    "        self.vocab = self._create_vocab(corpus=corpus)\n",
    "        \n",
    "    def _create_vocab(self, corpus: list[str]) -> dict[str, int]:\n",
    "        ...\n",
    "    \n",
    "    def _tokenize_document(self, document: str) -> list[int]:\n",
    "        ...\n",
    "    \n",
    "    def tokenize(self, documents: list[str]) -> list[list[int]]:\n",
    "        return [self._tokenize_document(document) for document in documents]\n",
    "\n",
    "class WordLevelTokenizer(Tokenizer):\n",
    "    def __init__(self, corpus: list[str]):\n",
    "        super().__init__(corpus=corpus)\n",
    "        \n",
    "    def _create_vocab(self, corpus: list[str]) -> dict[str, int]:\n",
    "        all_word_tokens = set([token for sample in corpus for token in sample.split(\" \")])\n",
    "        vocab = {token: index for index, token in enumerate(word_level_tokens, start=1)} \n",
    "        return vocab\n",
    "    \n",
    "    def _tokenize_document(self, document: str) -> list[int]:\n",
    "        return [self.vocab.get(token, -1) for token in document.split(\" \")]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb47327",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = os.path.join(data_dir, \"word_tokenizer.pkl\")\n",
    "\n",
    "with open(tokenizer_path, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc4007",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c973d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw[\"clean_stemmed\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bd3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = tokenizer.tokenize(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47171104",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(tokenized_tweets)\n",
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49062956",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c662e8",
   "metadata": {},
   "source": [
    "## One Hot (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11d4d4",
   "metadata": {},
   "source": [
    "Desired shape is: `[num_samples, vocab_size]`\n",
    "\n",
    "1. Define matrix of the desired shape, filled with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505d7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_onehot = np.zeros((num_samples, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36add2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sample in enumerate(tokenized_tweets):\n",
    "    for token in sample:\n",
    "        bow_onehot[index, token] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dcc13",
   "metadata": {},
   "source": [
    "## Count Encoding (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f003c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Count Encoding\n",
    "bow_count = np.zeros(()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce745a",
   "metadata": {},
   "source": [
    "## One Hot Encoding (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec6205",
   "metadata": {},
   "source": [
    "Desired Shape: `[num_samples, max_length, vocab_size]`\n",
    "\n",
    "> Padding is required\n",
    "\n",
    "But first let's implement, creating a matrix for each document with shape: `[len_document, vocab_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8957f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_seq = []\n",
    "for sample in tokenized_tweets:\n",
    "    doc_matrix = np.zeros((len(sample), vocab_size))\n",
    "    for token_index, token in enumerate(sample):\n",
    "        doc_matrix[token_index, token] = 1\n",
    "    ohe_seq.append(doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d6c01",
   "metadata": {},
   "source": [
    "### Determine Max Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcd57d",
   "metadata": {},
   "source": [
    "In order to create a matrix of fixed dimensions, we need to set a fixed length for all sentences aliased as `max_length`\n",
    "\n",
    "There are two approaches to determine this value:\n",
    "1. Find the document with the maximum length\n",
    "2. Find the *nth* percentile (95th, 98th, ..etc) for the docuemnts' lengths\n",
    "\n",
    "The second approach is recommended, since it takes into account outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c4f4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs_lengths = [len(document) for document in tokenized_tweets]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "\n",
    "sns.boxplot(y=docs_lengths, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d46011",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = int(np.percentile(a=docs_lengths, q=98))\n",
    "pad_idx = tokenizer.vocab.get('[PAD]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254b218",
   "metadata": {},
   "source": [
    "One approach to creating padded sequences is:\n",
    "1. Create the output matrix as all padding\n",
    "2. Limit documents with length greater than `max_length`\n",
    "3. Replace the padding vectors with real token vectors whenever found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c809d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "ohe_seq = np.zeros((num_samples, max_length, vocab_size))\n",
    "ohe_seq[:, :, pad_idx] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb0ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_index, document in enumerate(tokenized_tweets):\n",
    "    \n",
    "    # Step 2\n",
    "    document = document if len(document) <= max_length else document[0:max_length]\n",
    "    \n",
    "    # Step 3\n",
    "    for token_order, token in enumerate(document):\n",
    "        ohe_seq[doc_index, token_order, pad_idx] = 0\n",
    "        ohe_seq[doc_index, token_order, token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1dc66ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 21, 7755)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff6389",
   "metadata": {},
   "source": [
    "## One Hot Encoding Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d469dd1",
   "metadata": {},
   "source": [
    "Perform one hot encoding with a third party library\n",
    "\n",
    "Suggestions: \n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "- [torchtext](https://pytorch.org/text/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9598995",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b12ca495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazan/repos/nlp-notebooks/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:856: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[list([4451, 175, 4308, 5041, 1254, 4233, 2922, 1716, 645, 446, 5713, 7692, 5041, 7418, 7495, 835])\n list([3797, 3064, 5334, 5260, 1968, 5549, 4854, 6184, 2641, 4607, 4969, 5039, 4486, 6906])\n list([548, 3214, 1130, 3724, 6414, 6681, 4403, 4869, 6156, 5078, 6185, 1251, 1407, 3713, 4252, 7072, 707])\n ...\n list([6512, 934, 389, 13, 1749, 2727, 6706, 7276, 283, 6741, 3107, 3322, 6915, 1762, 1529, 5747, 7037])\n list([7666, 5456, 6472, 6624, 2685, 3265, 5652, 1138, 1692, 2789, 5280, 6305, 4233, 6119, 7207, 5114, 7034, 5280, 6447, 7133, 672, 4649])\n list([4543, 7376, 6145, 2430, 3663, 3505, 2902, 983, 5091, 7546, 3229, 1467, 5764, 6969, 4672])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_tweets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/nlp-notebooks/venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:818\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03mFit OneHotEncoder to X.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m    Fitted encoder.\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_keywords()\n\u001b[0;32m--> 818\u001b[0m fit_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infrequent_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infrequent_enabled:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_infrequent_category_mapping(\n\u001b[1;32m    826\u001b[0m         fit_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m], fit_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_counts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    827\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/nlp-notebooks/venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:80\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 80\u001b[0m X_list, n_samples, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m n_features\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/repos/nlp-notebooks/venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:45\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mPerform custom check_array:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m- convert list of strings to object dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     X_temp \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(X_temp\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mstr_):\n\u001b[1;32m     47\u001b[0m         X \u001b[38;5;241m=\u001b[39m check_array(X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite)\n",
      "File \u001b[0;32m~/repos/nlp-notebooks/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    880\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    881\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    884\u001b[0m         )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[list([4451, 175, 4308, 5041, 1254, 4233, 2922, 1716, 645, 446, 5713, 7692, 5041, 7418, 7495, 835])\n list([3797, 3064, 5334, 5260, 1968, 5549, 4854, 6184, 2641, 4607, 4969, 5039, 4486, 6906])\n list([548, 3214, 1130, 3724, 6414, 6681, 4403, 4869, 6156, 5078, 6185, 1251, 1407, 3713, 4252, 7072, 707])\n ...\n list([6512, 934, 389, 13, 1749, 2727, 6706, 7276, 283, 6741, 3107, 3322, 6915, 1762, 1529, 5747, 7037])\n list([7666, 5456, 6472, 6624, 2685, 3265, 5652, 1138, 1692, 2789, 5280, 6305, 4233, 6119, 7207, 5114, 7034, 5280, 6447, 7133, 672, 4649])\n list([4543, 7376, 6145, 2430, 3663, 3505, 2902, 983, 5091, 7546, 3229, 1467, 5764, 6969, 4672])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "encoder.fit(tokenized_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcdfda1",
   "metadata": {},
   "source": [
    "## TF-IDF (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111b08a",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. For the corpus calculate document frequency for each token (DF)\n",
    "2. For the corpus calculate inverse of document frequency for each token (IDF)\n",
    "3. For each document calculate term frequency (TF) for each token\n",
    "4. Calculate TF-IDF\n",
    "\n",
    "Desired output shape: `[num_samples, vocab_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66a9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
